---
title: "Spread and Source Detection of Wheat Stripe Rust in an Experimental Framework"
subtitle: "Simulation Results"
author:   
  - name: Trent VanHawkins
    affiliations:
      - ref: osu
    degrees: MS
    corresponding: true
  - name: Christopher Mundt
    affiliations:
      - ref: osu
    degrees: PhD
  - name: David Gent
    affiliations:
      - ref: usda
      - ref: osu
    degrees: PhD
  - name: Shirshendu Chatterjee
    affiliations:
      - ref: nyu
    degrees: PhD
  - name: Sharmodeep Bhattacharyya
    affiliations:
      - ref: osu
    degrees: PhD
affiliations:
  - id: osu
    name: Oregon State University
  - id: usda
    name: U.S. Department of Agriculture
  - id: nyu
    name: City University of New York
format: 
  html:
    lightbox: auto
embed-resources: true
toc: true
code-fold: true
number-sections: true
execute: 
  warning: false
code-links:
  - text: GitHub Repository
    href: https://github.com/hawktre/wheat_striperust
page-layout: full
references: references.bib
bibliography: references.bib
---

# Introduction

Previous reports for this project have documented the development and implementation of a model for the spread of Stripe Rust (*Puccinia striiformis*) among Wheat plants, using infection data collected from an experimental study conducted between April and July of 2024 at the Hyslop Crop Science Field Laboratory in Benton County, Oregon. The reports in the table below will recall the work that has been accomplished up to this point in chronological order.

| Report Name | Description |
|----------------------------|--------------------------------------------|
| 00_EDA | Exploratory Data Analysis |
| 00a_ModelingConsiderations | Modeling framework and derivations for non-linear Beta regression, accounting for zero-inflation |
| 04_ModelResults_Hurdle_logistic | Preliminary modeling results for the spread of the disease |
| 05a_SourceDetection_ModelConsiderations | Modeling framework and derivations for a source-detection model built upon the forward model. |
| 05b_BackwardModel.qmd | Implementation and results from the Backward Model using the dataset from the present study. |

In the previous report, 05b_BackwardModel, we implemented a framework for source detection (also referred to as a 'backwards model') by leveraging the optimized model parameters from the 'forward' model in an Expectation-Maximization (EM) optimization scheme, and presented preliminary results using the dataset from the present study.

Here, we check the accuracy and validity of the proposed model in a simulation study. Specifically, we simulate data using the learned parameters from previous reports. We then test whether the model can recover these parameters and accurately predict the source of disease outbreaks

# Methods

We have extensively documented preliminary analyses and model derivations in previous reports. Here, we briefly highlight those aspects of experimental design and model development which are essential to interpreting source-detection results.

## Experimental Design

This experiment was conducted between the months of April and July of 2024 at the Hyslop Crop Science Field Laboratory in Benton County, Oregon. Wheat was planted in one of four blocks (labeled A-D), each containing three replicates (plots). Within each block, each $30.5 \times 30.5$ m plot was inoculated with spores from *Puccinia striiformis* at one, two, or four locations of equal size ($0.76 \times 0.76$ m) between April 9th and April 25th, 2024. After approximately one month, plots were surveyed weekly for **five** consecutive weeks beginning May 17th, 2024. At each survey, domain experts visually estimated disease prevalence in $1.52 \times 1.52$ m grids, expressed as a percentage (0-100) of infected plant tissue within the sampling grid. Individual sampling locations are distinguished by their distance from the plot origin in meters. Further details of the experimental design may be found on pages 6 & 7 of the EEID Project Grant.

## Data Acquisition, Cleaning, and Exploration

Survey data, along with inoculum locations were provided by Chris Mundt. Additional data capturing wind speed (mph) and direction (degrees azimuth) were acquired from a local weather station \<1 km from the survey locations at a 15-minute resolution for the duration of the study period.

## Simulation Procedures

As was documented in previous reports, we have estimated the parameter vector $\boldsymbol\theta = \begin{bmatrix} \boldsymbol\tau & \phi\end{bmatrix}^\top = \begin{bmatrix} \beta & \delta & \gamma & \kappa & \phi\end{bmatrix}^\top$ using a non-linear beta-regression framework as is specified by @eq-meanmod.

$$
\log\left(\frac{\mu_{i,t}}{1-\mu_{i,t}}\right) = \eta_{i,t} = \beta+ \delta y_{i,t-1}(1 - y_{i,t-1}) + \gamma\sum_{j \neq i}\left(y_{j,t-1}w_{ij} (d_{ij} + d_0)^{-\kappa}\right)
$$ {#eq-meanmod}

Recall that when we assume that disease intensity $y_{i,t}$ follows a $\text{Beta}(\mu_{i,t}, \phi_t)$ distribution, $\phi_t$ is a nuisance parameter that can likewise be estimated with maximum likelihood methods. Also recall that we established zero-inflation within the experimental framework, as many plants did not have detectable levels of disease at early timepoints. We accounted for this zero-inflation process using a hurdle-model approach in which we estimate the parameter $\hat\pi_{i,t}$ for each block-treatment-visit combination, which characterizes a Bernoulli distribution governing the number of non-infected plants.

To assess any potential bias in our modeling framework and any potential downstream effects, we ran a simulation study with the following steps. For each block-treatment-visit combination:

1.  Simulate $n = 64$ observations from a $\text{Beta} \left( \hat\mu_i, \hat\phi \right)$ distribution.
2.  Simulate non-diseased plants from a $\text{Binomial}(n, \hat\pi)$.
3.  Fit the forward model as described in `00a_ModelingConsiderations` to estimate $\boldsymbol{\hat\theta}^{(k)}$.
4.  Using $\boldsymbol{\hat\theta}^{(k)}$ as an initialization, fit the backward model to estimate $\hat{p}_{i, s}^{(k)}$ --- the probability that plant $i$ was infected by source-group $s$.
5.  Aggregate elements of $\boldsymbol{\hat p}$ to predict the source of infection as is described in `05b_BackwardModel`.
6.  Repeat steps 1-5 10,000 times (K = 10,000).

Additional details on model initialization, fitting, and subsequent prediction can be found in the relevant reports as listed in the table above.

## Metrics for Model Evaluation

We primarily evaluate the forward model for bias in parameter estimation, and the backward model for its predictive ability.

### Forward Model

For the forward model, we specifically measure bias as

$$
\text{Bias}^{(k)} = \boldsymbol{\hat\theta}^{(k)} - \boldsymbol{\hat\theta}
$$ 

That is, we compute the difference between the estimated parameter vector in simulation $k \in K$ and the parameter vector estimated from the experimental data. We additionally kept track of whether the model converged and, if so, how many iterations were required using the quasi-Newtonian optimization algorithm (`optim(method = "BFGS")`). We finally report summaries of the bias for each parameter.

### Backward Model

Recall that the goal of the backward model is to compute the posterior probability that plant $i$ was infected by source group $s \in S$. This model naturally assumes that only one of the possible source groups is responsible for infection at any particular transition (e.g. a single plant $i$ cannot be infected by more than one source group). We accomplished this goal using an Expectation-Maximization (EM) approach which iteratively updates source-assignment probabilities ($\hat p_i^{(s)}$) and re-estimates $\theta$ until convergence.: 

$\textbf{E-step:}$ Use Bayes' rule to update 

$$
\hat p_i^{(s)} =  P(Z_i = s \mid \mathcal{Y}) \propto f(y_{i,t} \mid \mu_{i,t}^{(s)}, \phi_t) \cdot \pi_s
$$ {#eq-estep}

Where $\pi_s$ is the prior probability that source group $s$ is responsible for infection at unit $i$. We assume a uniform prior over all $S$ source groups $( \pi_s = 1/S \text{ for all } s)$.

$\textbf{M-step:}$ Maximize the expected complete-data log-likelihood $$
Q(\boldsymbol\theta) = \sum_{i=1}^n \sum_{s=1}^S \hat p_i^{(s)} \cdot \log f(y_{i,t} \mid \mu_{i,t}^{(s)}, \phi_t)
$$ {#eq-mstep}

In the case that the assumption of one source of infection was met (a single inoculation point), we defined prediction error as the the distance from the centroid of the predicted source to the centroid of the true source. Additionally, we defined a distance-weighted accuracy score as

$$
\text{score}_i = 1 - \frac{d_{\text{pred}}}{d_{\text{max}}}
$$

Where $d_{\text{pred}}$ is the distance-based error and $d_{\text{max}}$ is the largest possible distance from the true source within the plot. Note that the score is bounded between 0 (worst) and 1 (best). This metric may give us a slighlty better idea of the method's performance than traditional accuracy, since a wrong prediction that is *closer* to the true source is better than a wrong prediction that is *further* from the true source.

# Results

```{r}
library(tidyverse)
library(data.table)
library(here)
library(kableExtra)

# Read in the data
forward <- readRDS(here("DataProcessed/results/forward_model/forward_fits.rds"))
backward <- readRDS(here("DataProcessed/results/backward_model/backwards_fits.rds"))
sims <- readRDS(here("DataProcessed/results/simulation/sims_appended.rds"))
mod_dat <- readRDS(here("DataProcessed/experimental/mod_dat_arrays.rds"))

# Summarise convergence results
forward_sims <- sims %>% select(sim, block, treat, visit, converged.forward, neg_loglik.forward,iters, grad_norm, init_kappa, theta) %>% distinct()

# Repeat convergence summary for backwards fit
backward_sims <- sims %>% 
  select(sim, config, block, treat, visit, converged.forward, converged.backward, neg_loglik.backward, em_iters, predicted_source, true_source, dist_error, dist_acc) 
```

Overall, there were no convergence failures in the forward model. Some configurations in the backward model showed high failure rates---particularly when there was \>1 true source. This behavior is somewhat expected, as the backward model assumes one true source. It is perhaps more interesting to look at the failures for the replicates where model assumptions are satisfied (@tbl-back_converge_single).

```{r}
#| label: tbl-back_converge_single
#| tbl-cap: "Convergence failures in the backward model (Treatment = 1) stratified by configuration, block, treatment-level, and visit. Results have been filtered to only show replicates which failed to converge in > 1% of simulations."

backward_not_converged <- backward_sims %>% 
  filter(converged.forward & !converged.backward) %>% 
  group_by(config, block, treat, visit) %>% 
  summarise(N = n(), 
            prop = (n()/10000)*100, .groups = "drop") %>% 
  arrange(desc(prop)) %>% 
  filter(prop > 1) 
# Table for backwards fits convergence
backward_not_converged %>% 
  kableExtra::kable(digits = 3, col.names = c("Configuration", "Block", "Treatment", "Visit", "Failed to Converge (N)", "Failed to Converge (%)")) %>% 
  kableExtra::kable_styling(
    bootstrap_options = c("hover", "condensed", "responsive")
  ) %>% 
  kableExtra::row_spec(row = which(backward_not_converged$treat == "1" & backward_not_converged$prop > 5), background = "#ffcccc")
  
```

The failure rate exceeded exceeded 15\% in some multi-source settings, but largely remained below 2\% when the single-source assumption was met. There was one exception to this trend in which Block-C, Visit 4 failed in 10.6\% of simulations under the prior assumption of 16 potential source groups. It is possible that this failure is due to a weak disease gradient in the experimental data. Specifically, there is a large jump in intensity across the entire plot from Visit 3 to Visit 4 (@fig-blockCvisit4). 

```{r}
#| label: fig-blockCvisit4
#| fig-cap: "Spatial representation of the experimental data from Block: C, Visits: 3 & 4."
library(wesanderson)
library(sf)
stripe_dat <- readRDS(here("DataProcessed/experimental/stripe_clean.rds"))

intensity_cols <- wes_palette("Zissou1", 7, "continuous")

stripe_map <- stripe_dat %>% 
  select(block, inoculum_total, visit, north, east, intensity) %>% 
  mutate(visit = parse_number(visit), 
         intensity = intensity/100) %>% 
  rename(treat = inoculum_total) %>% 
  st_as_sf(coords = c("east", "north"))

stripe_map %>% 
  filter(block == "C", treat == 1, visit %in% c(3,4)) %>% 
  ggplot()+
  geom_sf(aes(color = intensity), size = 2)+
  facet_wrap(~visit) +
  labs(color = "Intensity", 
       title = "Block: C, Visits: 3 & 4 (Experimental Data)") +
  lims(color = c(0,1))+
  theme_bw() +
  scale_color_gradientn(colours = intensity_cols)
```


## Forward Model

In the forward model, we are primarily concerned with bias in the parameter estimates. @fig-forward-bias shows the distribution of parameter estimates across all simulations for Block A, stratified by treatment-level and visit. The colored point indicates the true value of the parameter.

```{r}
#| label: fig-forward-bias
#| fig-cap: "Forward model parameter estimates, stratified by block, treatment-level, and visit. The colored point indicates the true value of the parameter."
#| fig-width: 10

greek_cols <- RColorBrewer::brewer.pal(5, "Set1")

forward_sims_long <- forward_sims %>%
  filter(converged.forward == T) %>% 
  # Convert each named theta vector into a tibble (with name = param)
  mutate(theta = map(theta, ~ enframe(.x, name = "param", value = "value"))) %>%
  # Unnest into long format
  unnest(theta)

true_params <- forward %>% 
  mutate(theta = map(theta, ~ enframe(.x, name = "param", value = "value"))) %>%
  # Unnest into long format
  unnest(theta) %>% 
  select(block, treat, visit, param, value) %>% 
  mutate(treat = treat,
         visit = visit)
  
forward_sims_long <- left_join(forward_sims_long, true_params, by = c("block", "treat", "visit", "param"), suffix = c(".sim", ".true"))

plot_forward_sims <- function(blk, drop_gamma = F){
  if (drop_gamma) {
    forward_sims_long %>%
      mutate(treat_lab = paste0(treat," Source(s)"),
             visit_lab = paste0("Visit ", visit)) %>% 
      filter(block == blk & param != "gamma") %>%
      ggplot(aes(x = param, y = value.sim)) +
      geom_boxplot(outlier.alpha = 0.3) +
      geom_point(aes(y = value.true, color = param), 
                 shape = 18, size = 3, position = position_dodge(width = 0.75)) +
      facet_grid(visit_lab ~ treat_lab, scales = "free_y") +
      labs(y = "Estimated value", x = "Parameter", color = "True Value",
       title = paste0("Forward Model Simulated Parameter Estimates (Block ", blk, ")")) +
      theme_bw()+
      scale_color_manual(values = greek_cols[c(1,2,4,5)])+
      theme(legend.position = "none")
  }else{
  forward_sims_long %>%
      mutate(treat_lab = paste0(treat," Source(s)"),
             visit_lab = paste0("Visit ", visit)) %>% 
      filter(block == blk) %>%
      ggplot(aes(x = param, y = value.sim)) +
      geom_boxplot(outlier.alpha = 0.3) +
      geom_point(aes(y = value.true, color = param), 
                 shape = 18, size = 3, position = position_dodge(width = 0.75)) +
      facet_grid(visit_lab ~ treat_lab, scales = "free_y") +
      labs(y = "Estimated value", x = "Parameter", color = "True Value",
           title = paste0("Forward Model Simulated Parameter Estimates (Block ", blk, ")")) +
      theme_bw()+
      scale_color_manual(values = greek_cols)+
      theme(legend.position = "none")
  }
}

plot_forward_sims("A")
```

What we see in @fig-forward-bias is highly variable and frequently biased estimation of $\gamma$, the parameter governing cross-infection. To get a better idea of bias in the remaining parameters, we can remove $\hat\gamma$ to correct the scale of the plot. 

```{r}
#| label: fig-nogamma
#| fig-cap: "Boxplot of parameter estimates, with $\\gamma$ removed to correct the scale of the y-axis."
#| fig-width: 10

plot_forward_sims("A", drop_gamma = T)
```

@fig-nogamma shows that estimates are often still highly biased---particularly for the variance parameter $\phi$. Consistent underestimation of this parameter, as seen above in @fig-nogamma, could lead to inflated variance estimates. Surprisingly, the nonlinear parameter $\kappa$ is often estimated with low bias (@tbl-forwardbias). It is possible that this behavior is due to the model selection strategy (see `00a_ModelingConsiderations`), in which we optimize for 10 proposed initialization values of $\kappa$ and select the fit with the smallest finite negative log-likelihood. This approach may favor less biased values of $\hat\kappa$ at the expense of large values of $\hat\gamma$, reflecting the strong interplay between these two parameters: when the dispersal kernel shape ($\kappa$) is well identified, the cross-infection magnitude ($\gamma$) is left to absorb residual misspecification, resulting in inflated and unstable estimates of $\hat\gamma$.

```{r}
#| label: tbl-forwardbias
#| tbl-cap: "Bias computed as $\\hat\\theta_\\text{sim} - \\hat\\theta_\\text{true}$. Positive values indicate overestimation and negative values underestimation--on average."

tbl_bias <- forward_sims_long %>% 
  mutate(bias = value.sim - value.true) %>% 
  group_by(block, treat, visit, param) %>% 
  summarise(mean_bias = mean(bias),
            med_bias = median(bias),
            sd_bias = sd(bias), .groups = "drop")

tbl_bias %>% 
  filter(block == "A") %>% 
  arrange(desc(abs(med_bias))) %>% 
  kable(digits = 3, col.names = c("Block", "Treatment", "Visit", "Parameter", "Average Bias", "Median Bias", "Std. Dev.")) %>% 
  kableExtra::kable_styling(
    bootstrap_options = c("hover", "condensed", "responsive")
  )
```

Plots and tables for the remaining blocks can be found in @sec-appendix. Reported trends are similar across the remaining experimental replicates. 

## Backward Model

Although we conduct re-estimation of our parameter values from the forward model in the backward model using an EM-style algorithm, we are primarily interested in predictive performance. Recall for this section, that we currently have only defined predictive metrics in the case where there is one true inoculation point (Treatment = 1). 

```{r}
#| label: fig-spdensity
#| fig-width: 10
#| fig-cap: "Density of predictions overlayed with spatial grid and true inocuulation point for block A. Transparent or white labels indicates no predictions."
#| lightbox: 
#|   group: blockApreds

library(sf)
library(wesanderson)
## Read in Spatial Data
clusters <- readRDS(here("DataProcessed/experimental/clusters.rds"))
inocs <- readRDS(here("DataProcessed/experimental/inoc_sp.rds"))

#Set color scheme 
colors <- wes_palette("Zissou1", 7, type = "continuous")
# Clean up inocs 
inocs_clean <- inocs %>% select(block, inoculum_total, geometry) %>% 
  rename(treat = inoculum_total) %>% 
  filter(treat == 1)

## Create the Grid
grid <- bind_rows(clusters$stripe_4$grid,
                  clusters$stripe_8h$grid,
                  clusters$stripe_8v$grid,
                  clusters$stripe_16$grid) %>% 
  mutate(configuration = case_when(grid_type == "2 x 2" ~ "stripe_4",
                                   grid_type == "4 x 2" ~ "stripe_8h",
                                   grid_type == "2 x 4" ~ "stripe_8v",
                                   grid_type == "4 x 4" ~ "stripe_16")) %>% 
  rename("geometry" = ".")

## Record Centroids
centroid <- bind_rows(clusters$stripe_4$centroid,
                  clusters$stripe_8h$centroid,
                  clusters$stripe_8v$centroid,
                  clusters$stripe_16$centroid) %>% 
  mutate(configuration = case_when(grid_type == "2 x 2" ~ "stripe_4",
                   grid_type == "4 x 2" ~ "stripe_8h",
                   grid_type == "2 x 4" ~ "stripe_8v",
                   grid_type == "4 x 4" ~ "stripe_16")) %>% 
  rename("geometry" = ".") %>% 
  mutate(config = str_split_i(configuration, "_", 2)) %>% 
  select(config, grid_type, grid_id, geometry)

#Subset relevant results
preds_sp <- backward_sims %>% 
  filter(treat == 1 & converged.backward == T) %>% 
  left_join(centroid, by = c("config", "predicted_source" = "grid_id")) %>% 
  st_as_sf()

preds_sp_summary <- preds_sp %>% 
  group_by(config, grid_type, block, treat, visit, predicted_source) %>% 
  summarise(n = n(),
            prop = n()/10000, .groups = "drop") %>% 
  mutate(visit = paste0("Visit ", visit))

#Create a plot for Block A
preds_plt <- function(cfg, blk){
  cur.config <- preds_sp_summary %>% filter(config == cfg, block == blk)
  cur.gridtype <- unique(cur.config$grid_type)
  cur.centroid <- centroid %>% filter(config == cfg)
  
  ggplot() +
    geom_sf_label(data = cur.centroid, aes(label = grid_id))+
    geom_sf_label(data = cur.config, aes(label = predicted_source, fill = prop))+
  geom_sf(data = grid %>% filter(grid_type == cur.gridtype), fill = "transparent")+
  geom_sf(data = inocs_clean %>% filter(block == blk), 
          size = 2, 
          shape = 23,
          fill = "#F8766D") +   
  facet_wrap(~visit, nrow = 1)+
  labs(x = "East", y = "North", fill = "Prop. Predicted",
       title = paste0("Spatial Prediction Density for Block ", blk, " (", cur.gridtype, ")"))+
  scale_fill_gradientn(colors = colors, limits = c(0,1))+
  theme_classic() +
  theme(legend.position = "bottom")
}

preds_plt("4", "A")
preds_plt("8h", "A")
preds_plt("8v", "A")
preds_plt("16", "A")
```

What we observe in the figures above and @tbl-predmets is strong predictive ability in earlier visits (2 & 3) with tapering predictive ability in later visits (4 & 5). This trend is particularly strong when the true source falls near the boundary of two potential source groups (4 x 2 and 4 x 4 configurations). This result is an indication that distinction between sources becomes more challenging as the experimental plot becomes inundated with the disease. On average, predictions were < 3m from the true source. Conditioning upon the incorrect predictions, we see more typically that incorrect predictions were ~5m from the true source but note that large errors are based on very few incorrect predictions in some cases. 

```{r}
#| label: tbl-predmets
#| tbl-cap: "Predictive scores and error for Block A--visits 2-5 under each of the four prior specifications of the number of sources and their orientations. *Avg. Error* was calculated using all 10,000 simulations and *Avg. Miss Error* was calculated using only simulations with incorrect predictions."
#| 
all_sp_summary <- preds_sp %>% 
  group_by(config, grid_type, block, treat, visit) %>% 
  summarise(incorrect_preds = sum(predicted_source != true_source),
            mean_acc = mean(dist_acc),
            mean_error = mean(dist_error),
            .groups = "drop") %>% 
  st_drop_geometry()

miss_error <- preds_sp %>% 
  filter(dist_error != 0) %>% 
  group_by(config, grid_type, block, treat, visit) %>% 
  summarise(mean_error_miss = median(dist_error),
            .groups = "drop") %>% 
  st_drop_geometry()

truth_sp_summary <- left_join(all_sp_summary, miss_error, by = c("config", "grid_type", "block", "treat", "visit"))

sp_tbl <- function(blk){
  truth_sp_summary %>% 
  filter(block == blk) %>% 
  select(grid_type, block, visit, mean_acc, incorrect_preds, mean_error, mean_error_miss) %>% 
  st_drop_geometry() %>% 
  arrange(desc(mean_error)) %>% 
  kable(digits = 4, col.names = c("Configuration", "Block", "Visit", "Avg. Distance-weighted Accuracy", "Incorrect Predicitons (N)", "Avg. Error (m)", "Avg. Miss Error (m)")) %>% 
  kableExtra::kable_styling(
    bootstrap_options = c("hover", "condensed", "responsive")
  )
}

sp_tbl(blk = "A")
```

# Discussion

In this report, we reviewed the efficacy of the present method for unbiased estimation of parameters from the forward-model and the predictive efficacy of the backward-model using 10,000 simulations. The primary finding was that the model was highly predictive of the true disease source when the assumptions (one true source) were met---despite highly biased estimation from the forward model. Predictive efficacy of the backward model was strong in earlier visits, occurring in late May and early June, when the disease gradient was strongest. Predictive ability then tapered as the disease progressed across the plot and spatial signal became more diffuse, especially when the inoculation point fell near the boundary of two source groups. 

While forward-model estimates were often highly biased, they clearly served as sufficient initialization for the backward-model in many cases. Notably, estimates of the cross-infection term $\gamma$ and the nuisance variance parameter $\phi$ often showed large bias. Interestingly, the non-linear parameter $\kappa$ was often captured by the forward-model, which we attribute to the initialization strategy in which we perform model selection across 10 proposed initial values. 

Future work may focus on the single-source assumption so that we may reliably make predictions in the more realistic cases where there were either two or four inoculation points of the disease and a fast-moving epidemic. In the present study, we saw that the most common convergence failures for our model occurred in these situations, where the single-source assumption was violated. In particular, the model often failed in later visits--when there were high levels of disease and a weak disease gradient. Some possible solutions may involve a similar EM-style algorithm which can account for the possibility of 'mixed membership' (e.g. a single plant may be infected by multiple source groups). 

# Appendix {#sec-appendix}

## Simulated Forward Model Parameter Estimates

### All Estimates

```{r}
#| label: fig-forward-bias-appendix
#| fig-width: 10
#| lightbox: 
#|   group: forwardparams
plot_forward_sims("A")
plot_forward_sims("B")
plot_forward_sims("C")
plot_forward_sims("D")
```

### Parameter Estimates ($\gamma$ removed for scale)

```{r}
#| label: fig-forward-bias-appendix-nogamma
#| fig-width: 10
#| lightbox: 
#|   group: forwardparamsnogamma
plot_forward_sims("A", drop_gamma = T)
plot_forward_sims("B", drop_gamma = T)
plot_forward_sims("C", drop_gamma = T)
plot_forward_sims("D", drop_gamma = T)
```

### Spatial Predictions

#### Block B
```{r}
#| label: fig-spdensity-appendixB
#| fig-width: 10
#| fig-cap: "Density of predictions overlayed with spatial grid and true inocuulation point for Block B. Transparent or white labels indicates no predictions."
#| lightbox: 
#|   group: blockBpreds

preds_plt("4", "B")
preds_plt("8h", "B")
preds_plt("8v", "B")
preds_plt("16", "B")
```

```{r}
#| label: tbl-predmetsB
#| tbl-cap: "Predictive scores and error for Block B--visits 2-5 under each of the four priors for number of source groups and orientation."

sp_tbl("B")
```

#### Block C
```{r}
#| label: fig-spdensity-appendixC
#| fig-width: 10
#| fig-cap: "Density of predictions overlayed with spatial grid and true inocuulation point for Block C. Transparent or white labels indicates no predictions."
#| lightbox: 
#|   group: blockCpreds

preds_plt("4", "C")
preds_plt("8h", "C")
preds_plt("8v", "C")
preds_plt("16", "C")
```

```{r}
#| label: tbl-predmetsC
#| tbl-cap: "Predictive scores and error for Block C--visits 2-5 under each of the four priors for number of source groups and orientation."

sp_tbl("C")
```

#### Block D
```{r}
#| label: fig-spdensity-appendixD
#| fig-width: 10
#| fig-cap: "Density of predictions overlayed with spatial grid and true inocuulation point for Block D. Transparent or white labels indicates no predictions."
#| lightbox: 
#|   group: blockDpreds

preds_plt("4", "D")
preds_plt("8h", "D")
preds_plt("8v", "D")
preds_plt("16", "D")
```

```{r}
#| label: tbl-predmetsD
#| tbl-cap: "Predictive scores and error for Block D--visits 2-5 under each of the four priors for number of source groups and orientation."

sp_tbl("D")
```