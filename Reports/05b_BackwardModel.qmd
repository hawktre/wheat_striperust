---
title: "Spread of Wheat Stripe Rust in an Experimental Framework"
subtitle: "Preliminary Results: A Model for Source Detection Using EM-algorithm"
author:   
  - name: Trent VanHawkins
    affiliations:
      - ref: osu
    degrees: MS
    corresponding: true
  - name: Christopher Mundt
    affiliations:
      - ref: osu
    degrees: PhD
  - name: David Gent
    affiliations:
      - ref: usda
      - ref: osu
    degrees: PhD
  - name: Shirshendu Chatterjee
    affiliations:
      - ref: nyu
    degrees: PhD
  - name: Sharmodeep Bhattacharyya
    affiliations:
      - ref: osu
    degrees: PhD
affiliations:
  - id: osu
    name: Oregon State University
  - id: usda
    name: U.S. Department of Agriculture
  - id: nyu
    name: City University of New York
format: 
  html:
    lightbox: auto
embed-resources: true
toc: true
code-fold: true
execute: 
  warning: false
code-links:
  - text: GitHub Repository
    href: https://github.com/hawktre/wheat_striperust
page-layout: full
references: references.bib
bibliography: references.bib
---

# Introduction

Previous reports for this project have documented the development and implementation of a model for the spread of Stripe Rust (*Puccinia striiformis*) among Wheat plants, using infection data collected from an experimental study conducted between April and July of 2024 at the Hyslop Crop Science Field Laboratory in Benton County, Oregon. The reports in the table below will recall the work that has been accomplished up to this point in chronological order.

| Report Name | Description |
|----------------------------|--------------------------------------------|
| 00_EDA | Exploratory Data Analysis |
| 00a_ModelingConsiderations | Modeling framework and derivations for non-linear Beta regression, accounting for zero-inflation |
| 04_ModelResults_Hurdle_logistic | Preliminary modeling results for the spread of the disease |
| 05a_SourceDetection_ModelConsiderations | Modeling framework and derivations for a source-detection model built upon the forward model. |

The remaining reports found in the 'reports' folder document incremental steps in the modeling procedure that culminate in the work produced in `04_ModelREsults_Hurld_logistic`.

In the previous report, 05a_SourceDetection_ModelConsiderations, we introduced a framework for source detection (also referred to as a 'backwards model') by leveraging the optimized model parameters from the 'forward' model in an Expectation-Maximization (EM) optimization scheme. Here, we present the initial results for this method using the present dataset.

# Methods

We have extensively documented exploratory analyses and model derivations in previous reports. Here, we briefly highlight those aspects of experimental design and model development which are essential to interpreting source-detection results.

## Experimental Design

This experiment was conducted between the months of April and July of 2024 at the Hyslop Crop Science Field Laboratory in Benton County, Oregon. Wheat was planted in one of four blocks (labeled A-D), each containing three replicates (plots). Within each block, each $30.5 \times 30.5$ m plot was inoculated with spores from *Puccinia stiiformis* at one, two, or four locations of equal size ($0.76 \times 0.76$ m) between April 9th and April 25th, 2024. After approximately one month, plots were surveyed weekly for **five** consecutive weeks beginning May 17th, 2024. At each survey, domain experts visually estimated disease prevalence in $1.52 \times 1.52$ m grids, expressed as a percentage (0-100) of infected plant tissue within the sampling grid. Individual sampling locations are distinguished by their distance from the plot origin in meters. Further details of the experimental design may be found on pages 6 & 7 of the EEID Project Grant.

## Data Acquisition, Cleaning, and Exploration

Survey data, along with inoculum locations were provided by Chris Mundt. Additional data capturing wind speed (mph) and direction (degrees azimuth) were acquired from a local weather station \<1 km from the survey locations at a 15-minute resolution for the duration of the study period.

## Backward Model

As in @ojwang2021, let us consider a latent variable $Z_i$ indicating the source of infection for plant $i$:

$$
Z_i \sim \text{Multinomial}(1, \boldsymbol{p}_i)
$$

Where

-   $\boldsymbol{p}_i = \left(p_{i1}, \ldots, p_{iS}\right)$
-   $p_{is} = P(Z_i = s)$
-   $\sum_{s=1}^Sp_{is} = 1$

Define the complete data as $\mathcal{X} = \left(\mathcal{Y}, \boldsymbol{Z}\right)$, where $\mathcal{Y} = \left(y_{i,t}, y_{j,t-1}, w_{ij}, d_{ij}\right)$ for all $j \neq i$. The complete-data log-likelihood is then given by:

$$
\ell(\boldsymbol{\theta} \mid \mathcal{X}) = \sum_{i=1}^n \sum_{s=1}^S \mathbb{1} \{Z_i = s\} \cdot \log f(y_{i,t} \mid \mu_{i,t}^{(s)}, \phi_t)
$$ {#eq-loglik}

where $\mu_{i,t}^{(s)}$ denotes the predicted mean intensity at $i$ assuming infection from source group $s$. We can then apply the Expectation-Maximization (EM) algorithm to estimate the posterior source probabilities $\hat p^{(s)}$. At each iteration:

$\textbf{E-step:}$ Use Bayes' rule to update 

$$
\hat p_i^{(s)} =  P(Z_i = s \mid \mathcal{Y}) \propto f(y_{i,t} \mid \mu_{i,t}^{(s)}, \phi_t) \cdot \pi_s
$$ {#eq-estep}

Where $\pi_s$ is the prior probability that source group $G_s$ is responsible for infection at unit $i$. We assume a uniform prior over all $S$ source groups $( \pi_s = 1/S \text{ for all } s)$.

$\textbf{M-step:}$ Maximize the expected complete-data log-likelihood 
$$
Q(\boldsymbol\theta) = \sum_{i=1}^n \sum_{s=1}^S \hat p_i^{(s)} \cdot \log f(y_{i,t} \mid \mu_{i,t}^{(s)}, \phi_t)
$$ {#eq-mstep}

This schematic can be made more complex by considering that dispersal dynamics may differ across source groups $s \in S$. Here, we will assume that dispersal dynamics are constant across groups. That is, we will continue to estimate a global parameter vector, $\boldsymbol\theta$ as opposed to a source-specific $\boldsymbol\theta^{(s)}$.

Note that this new modeling framework requires a prior on the number of sources $S$. We assigned each plant $i$ to one of $S$ evenly-divided and non-overlapping potential source groups $G_s \subset i \in \left\{1, 2, \ldots, n \right\}$ (@fig-sourcegroup).

```{r}
#| echo: false
#| message: false
#| label: fig-sourcegroup
#| fig-align: center
#| fig-cap: "Schematics for each of the four grouping strategies considered in this study; corresponding to 16, 8, and 4 plants per source group from left to right."
#| fig-width: 8

library(tidyverse)
library(here)
library(sf)

clusters <- readRDS(here("DataProcessed/experimental/clusters.rds"))

grid <- bind_rows(clusters$stripe_4$grid,
                  clusters$stripe_8h$grid,
                  clusters$stripe_8v$grid,
                  clusters$stripe_16$grid)


grid %>% 
  ggplot()+
  geom_sf(fill = "transparent")+
  geom_sf_text(aes(label = grid_id))+
  facet_wrap(~grid_type, nrow = 1)+
  labs(x = "E", y = "N")+
  theme_classic()
```

At each iteration, we update $\hat\mu_{i,t}^{(s)}$ as

$$
\hat\mu_{i,t}^{(s)} = \text{logit}^{-1}\left(\hat\eta_{i,t}^{(s)}\right) = \hat\beta + \hat\delta y_{i,t-1}(1 - y_{i,t-1}) + \hat\gamma \sum_{\{j \neq i\} \in G_s} \left(y_{j,t-1} w_{ij} (d_{ij} + d_0)^{-\hat\kappa}\right)
$$ {#eq-meanmod2}

Initial parameter values were provided as the optimized maximum likelihood estimates (MLEs) from the 'forward' model. Like the forward model, only non-zero observations at each time transition are used to optimize the parameter vector $\hat{\boldsymbol\theta}_t$. Finally, BIC was used to select the best prior number of sources.

## Prediction

The EM algorithm produces an $N \times S$ matrix for each time transition $t$ and each replicate $b$, $\hat{\boldsymbol p}$, where each entry $\hat p_{is}$ represents the posterior probability that plant $i$ was infected by source group $s$. To make results more interpretable, we can average over the groups as

$$
\bar{\boldsymbol{p}} = \frac{1}{|G_s|}\sum_{i \in G_s} p_{is}
$$ 

This is a matrix of size $S \times S$ where diagonal entries represent the probability of self-infection (within-group spread), while off-diagonal entries represent the probability of cross-group infection.


In the experimental treatment with a single true inoculation point, we use the estimated group-level probability matrix $\bar{\boldsymbol p}$ to predict the most likely source group of infection at each replicate-transition combination. Ignoring indices for replicate and transition, we can define the most probable source of infection for each group

$$
\hat s = \text{argmax} \ \bar{\boldsymbol p}
$$

There will be $B \times T$ of these predictions. If we define the *true* source as the one containing the inoculation point (in the applicable treatment level), we can use our predictions to compute a standard suite of multiclass prediction metrics such as accuracy, F-1 score, or Cohen's Kappa. Note, however, that the inoculation point sometimes lies near the border of two spatial groups. Traditional mutliclass metrics are produced from confusion matrices, where a *correct* prediction only occurs when the predicted class label matches the true class label. In this case, this approach may not produce a realistic, interpretable metric. To counter that, we propose a distance-weighted accuracy. Let $d_{\text{pred}}$ denote the distance from the centroid of the predicted source to the centroid of the true source and $d_{\text{max}}$ be the distance of the furthest centroid from the true source. Define the score for an individual prediction as

$$
\text{score}_i = 1 - \frac{d_{\text{pred}}}{d_{\text{max}}}
$$
When we have $N$ predictions, the distance-weighted accuracy is then given by 

$$
\frac{1}{N}\sum_{i = 1}^N \text{score}_i
$$

This will provide a more realistic interpretation of model accuracy, particularly in configurations with finer grids (e.g. 2x4, 4x2, 4x4). 

## Sensitivity Analysis
It is well understood that EM-style algorithms can be highly sensitive to different parameter initializations. To assess the sensitivity of our algorithm, we considered two intialization options. 

  1. **Informed:** Initialize the EM-algorithm with the optimized MLEs from the forward model. 
  2. **Naive:** Initialize the EM-algorithm using the grid-search technique employed to optimize the forward model. 
  
We then compare the parameter estimates and predictions for both initialization strategies. For more on the Naive initialization procedure, please see '00a_ModelingConsiderations'.

# Results

```{r}
library(tidyverse)
library(here)
library(sf)

mle_results <- readRDS(here("DataProcessed/results/backward_model/mle_results.rds"))
clusters <- readRDS(here("DataProcessed/experimental/clusters.rds"))
inocs <- readRDS(here("DataProcessed/experimental/inoc_sp.rds"))


# Model selection ---------------------------------------------------------
## Compute BIC
fit_df <- mle_results$params %>% 
  select(-c(param, value)) %>% 
  distinct() %>% 
  mutate(n_mix = parse_number(configuration),
         bic_k = (n_mix - 1 + 5),
         bic = bic_k * log(64) - 2 * log(abs(final_neg_loglik)))

## Subset the best models
best_mod <- fit_df %>%
  group_by(plot_id, visit) %>%
  slice_min(order_by = bic, n = 1, with_ties = FALSE) %>%
  ungroup() %>% 
  mutate(configuration = factor(configuration, 
                                levels = c("stripe_4", "stripe_8h", "stripe_8v", "stripe_16"), 
                                labels = c("2 x 2", "2 x 4", "4 x 2", "4 x 4")))
```

EM runs converged quickly for every treatment-replicate-visit combination (min = `r min(fit_df$em_iters)` iters, max = `r max(fit_df$em_iters)` iters), and the $2 \times 2$ configuration (4 source groups) almost always achieved the lowest BIC, with the exception of B4 - Visits 3 and 4, and D2-Visit 4. The result is somewhat expected as BIC penalizes heavily for the addition of mixture components. Quick convergence suggests the models ability to scale well, computationally. 

## Single-Source Detection

It is perhaps most practical to work with the scenarios in which there is a single inoculation point and true source group. Specifically, we can test our ability to predict the origin of the disease outbreak at each discrete time transition. It is easiest to conceptualize these predictions spatially as in @fig-2by2

```{r}
#| label: fig-2by2
#| layout-ncol: 1
#| fig-cap: "Spatial visualization of predicted infection probabilities for all replicate blocks with a single inoculation point and four possible source groups. Nodes indicate the probability of self-infection, while arrows indicate the probability of infection from another source. Directional edges are only displayed for the most probable source (if different from itself), with transparency encoding the weight $(\\hat p)$."
#| fig-subcap: 
#| - "Block A"
#| - "Block B"
#| - "Block C"
#| - "Block D"
#| lightbox: 
#|   group: single2by2


#Create single spatial grid object
grid <- bind_rows(clusters$stripe_4$grid,
                  clusters$stripe_8h$grid,
                  clusters$stripe_8v$grid,
                  clusters$stripe_16$grid) %>% 
  mutate(configuration = case_when(grid_type == "2 x 2" ~ "stripe_4",
                                   grid_type == "4 x 2" ~ "stripe_8h",
                                   grid_type == "2 x 4" ~ "stripe_8v",
                                   grid_type == "4 x 4" ~ "stripe_16")) %>% 
  rename("geometry" = ".")

#Create single spatial centroid object
centroid <- bind_rows(clusters$stripe_4$centroid,
                  clusters$stripe_8h$centroid,
                  clusters$stripe_8v$centroid,
                  clusters$stripe_16$centroid) %>% 
  mutate(configuration = case_when(grid_type == "2 x 2" ~ "stripe_4",
                   grid_type == "4 x 2" ~ "stripe_8h",
                   grid_type == "2 x 4" ~ "stripe_8v",
                   grid_type == "4 x 4" ~ "stripe_16")) %>% 
  rename("geometry" = ".")


#Plot it
library(wesanderson)
self_infection <- wes_palette("Zissou1", 7, type = "continuous")

config_match <- data.frame(name = c("stripe_4", "stripe_8h", "stripe_8v", "stripe_16"),
                           config = c("2 x 2", "2 x 4", "4 x 2", "4 x 4"))

nodes <- mle_results$preds$nodes
edges <- mle_results$preds$edges

spat_plots <- list()

for (config in unique(edges$configuration)) {
  for (plt in unique(edges$plot_id)) {
    config_type <- config_match %>% filter(name == !!config) %>% .$config
    
    spat_plots[[paste0(plt, "_", config)]] <- ggplot() +
      geom_sf_label(data = nodes %>% filter(plot_id == plt, configuration == config), aes(fill = preds, label = member_group))+
      geom_sf(data = edges %>% filter(plot_id == plt, configuration == config, is_max == 1, member_group != pred_group),
              aes(alpha = preds), 
              arrow = arrow(type = "closed", length = unit(0.075, "inches")))+
      geom_sf(data = grid %>% filter(configuration == config), fill = "transparent")+
      geom_sf(data = inocs %>% filter(plot_inoc_id == plt), size = 2, shape = 23, fill = "#F8766D") +
      facet_wrap(~visit, nrow = 1)+
      theme(legend.position = "bottom")+
      scale_alpha_continuous(limits = c(0, 1))+
      scale_fill_gradientn(colors = self_infection, limits = c(0,1)) + 
      labs(title = paste0("Spatial Network Predictions (", config_type, ", ", plt, ")"),
           fill = "Self Infection",
           alpha = "Dispersed Infection",
           x = "East",
           y = "North")+
      theme_classic() +
      theme(legend.position = "bottom")
  }
}

spat_plots[["A1_stripe_4"]]

ggsave("A1_Results.png", path = here("Reports/CAS_poster/"), width = 10, height = 4, units = "in")
spat_plots[["B1_stripe_4"]]
spat_plots[["C1_stripe_4"]]
spat_plots[["D1_stripe_4"]]
```

What we observe from @fig-2by2 is the consistent ability of the model to identify sources of infection through the **self-infection** probabilities. That is, the nodes of the true source group often display warm colors-- indicating a high probability of self-infection. It is worth noting that the model may also capture latent disease dynamics, which could lead to an incorrect prediction by our definition--in which the true source group is the one containing the inoculation point. We may also wish to look at our ability to predict at a finer scale. Here, we show spatial predictions for the $4 \times 4$ grid, but remaining plots can be found in the appendix. 

```{r}
#| label: fig-4by4
#| layout-ncol: 1
#| fig-cap: "Spatial visualization of predicted infection probabilities for all replicate blocks with a single inoculation point and sixteen possible source groups. Nodes indicate the probability of self-infection, while arrows indicate the probability of infection from another source. Directional edges are only displayed for the most probable source (if different from itself), with transparency encoding the weight $(\\hat p)$."
#| fig-subcap: 
#| - "Block A"
#| - "Block B"
#| - "Block C"
#| - "Block D"
#| lightbox: 
#|   group: single4by4

spat_plots[["A1_stripe_16"]]
ggsave("A1_results16.png", path = here("Reports/CAS_poster/"), width = 10, height = 4, units = "in")
spat_plots[["B1_stripe_16"]]
spat_plots[["C1_stripe_16"]]
spat_plots[["D1_stripe_16"]]
```
Similarly to @fig-2by2, we see the models generalized ability to identify a single true source in @fig-4by4. That is, the model predicts a high probability of self-infection either at the true source or at a neighbor to the true source. This finding highlights the need for a distance-weighted accuracy metric which can account for spatial error. We can see that it is more likely that the single inoculation point falls on or near a border as we consider source groups with smaller areas. Again, we can perhaps also see some latent disease dynamics at play through cross-infection probabilities (gray arrows). As the disease spreads across the plot, it may become more probable that groups are infected by groups other than the group containing the epidemic origin. 


```{r}
#| warning: false
#| label: tbl-predmets
#| tbl-cap: "Prediction metrics for each grouping configuration, when there is a single true disease source (e.g. Treatment Level = 1)."

library(tidymodels)
library(kableExtra)

# Summarise distance-weighted accuracy
dist_acc_mle <- bind_rows(mle_results$singlesource_acc) %>% 
  group_by(configuration) %>% 
  summarise(dist_acc = mean(dist_acc), .groups = "drop")

#Compute other prediction metrics
multi_metrics <- metric_set(accuracy, kap, f_meas)

mle_metrics <- mle_results$singlesource_acc %>% 
  imap_dfr(~ multi_metrics(.x, truth = true_group, estimate = pred_group, ) %>% 
                    mutate(configuration = .y), .id = NULL) 

# Join with distance-weighted acc
mle_metrics <- left_join(mle_metrics, dist_acc_mle, by = "configuration")%>% 
  select(-.estimator) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate)

 # Create the table 
mle_metrics %>% 
  select(configuration, dist_acc, accuracy) |> 
  as.data.frame() %>% 
  mutate(configuration = c("2 x 2", "2 x 4", "4 x 2", "4 x 4")) %>% 
  kable(col.names = c("Configuration", "Accuracy (Distance-Weighted)", "Accuracy (Unweighted)"), digits = 3, format = "latex")
   

```

Prediction metrics are formalized and displayed in @tbl-predmets. It is clear that a distance-weighted accuracy metric gives a slightly more favorable performance to a larger number of potential source groups ($2 \times 2$, $2 \times 4$, $4 \times 2$), but the $2 \times 2$ configuration displays the best predictive performance. This is outcome is expected--as we are predicting over the largest space when we have the least possible source groups.

## Multi-Source Prediction

More work is needed to derive formal prediction metrics for the multi-source prediction scenarios, but we can still visualize spatial prediction to get a qualitative sense of our model's ability to distinguish multiple disease foci. Let us anecdotally consider two scenarios: one in which multiple foci were placed relatively closely together and formed a contiguous disease wavefront (C4), and another where there are two distinct disease epicenters (B4). 

```{r}
#| label: fig-multifoci
#| layout-ncol: 1
#| fig-cap: "Spatial visualization of predicted infection probabilities a) when multiple foci are relatively close and b) when there are two distinct disease epicenters. Nodes indicate the probability of self-infection, while arrows indicate the probability of infection from another source. Directional edges are only displayed for the most probable source (if different from itself), with transparency encoding the weight $(\\hat p)$."
#| fig-subcap: 
#| - "Multiple foci form a contiguous disease wavefront."
#| - "Multiple foci form two distinct disease epicenters."
#| lightbox: 
#|   group: multifoci

spat_plots[["C4_stripe_16"]]
spat_plots[["B4_stripe_16"]]
```

Panel (a) of @fig-multifoci demonstrates a scenario in which multiple foci formed a contiguous disease wavefront (we know from the observed data in '00_EDA'), whereas Panel (b) of @fig-multifoci demonstrates a scenario in which we have multiple distinct disease epicenters. Observing the predictions from Panel (a), we see high probabilities of self-infection in some true source groups. In Panel (b), however, we see particularly in visits 3 and 4 that the model predicts equal probability of infection everywhere. Essentially, the model "doesn't know". This is likely due to the fact that the model, in its current form, assumes a single true disease origin. Thus, the assumption of the model is violated in Panel (b). It is interesting, however, that the model clearly has some distinguishing power in Panel (a), visit 5. 

## Sensitivity Analysis

The initialization strategy used above was to pass the optimized MLE estimates from the forward model to the backward model. We may hypothesize that this strategy is responsible for the relatively quick convergence of the backward model above. Here, we also consider a 'naive' initialization procedure, similar to the one used to initialize the forward model. That is, we grid-search for $\kappa$ to linearize the model and use OLS estimates as initial guesses for the remaining parameters. We then select the 'best' model using likelihood-based metrics. Here, we compare computational speed, parameter estimates, and predictive power of these two strategies. 

```{r}
#| label: fig-emiters
#| fig-cap: "The distribution of EM-algorithm iterations it took for each of the initialization strategies to converge."
discrete_cols <- wes_palette("Darjeeling1", n = 5, type = "discrete")
sensitivity_results <- readRDS(here("DataProcessed/results/backward_model/sensitivity_results.rds"))

sensitivity_iters <- sensitivity_results$params %>% 
  select(configuration, plot_id, visit, em_iters) %>% 
  distinct()

mle_iters <- mle_results$params %>% 
  select(configuration, plot_id, visit, em_iters) %>% 
  distinct()

left_join(mle_iters, sensitivity_iters, by = c("configuration", "plot_id", "visit"), suffix = c(".MLE", ".Naive")) %>% 
  pivot_longer(contains("em_iters"), names_to = "init_type", values_to = "em_iters") %>% 
  mutate(init_type = sapply(init_type, str_split_i, pattern = "\\.", i = 2)) %>% 
  ggplot(aes(x = em_iters))+
  geom_histogram(aes(fill = init_type), position = "identity", alpha = 0.7, color = "black")+
  labs(x = "EM Iterations", y = "Count", fill = "Initialization")+
  scale_fill_manual(values = discrete_cols)+
  theme_classic()
```

@fig-emiters shows that the grid-search (naive) initialization scheme has a slightly higher right skew for the number of EM-iterations until convergence, with some notable outliers. From a practical standpoint--runtimes are similar for the two initialization strategies. We compare parameter estimates in @fig-params. 

```{r}
#| label: fig-params
#| fig-cap: "Comparison of parameter values between the two initialization methods for Block C with the $$2 \\times 2$$ configuration. The dashed line denotes equal estimates."
#| fig-height: 8
#| fig-width: 10

params_diff <- sensitivity_results$params %>% 
  left_join(mle_results$param, 
            by = c("configuration", "plot_id", "visit", "param"),
            suffix = c(".sensitivity", ".mle")) %>% 
  mutate(p_diff = ((value.sensitivity - value.mle)/value.mle)* 100)

params_diff %>%
  filter(configuration == "stripe_4", str_detect(plot_id, "C")) %>%
  ggplot(aes(x = value.mle, y = value.sensitivity)) +
  geom_point(aes(color = param), size = 3)+
  geom_abline(slope = 1, intercept = 0, linetype = "dashed")+
  facet_grid(plot_id~visit) + 
  labs(title = "Parameter Comparison (Block C)",
       color = "Parameter",
       x = "Informed Estimate",
       y = "Nieve Estiamte")+
  theme(legend.position = "bottom")+
  theme_bw()+
  scale_color_manual(values = discrete_cols)
```

There are too many replicate-treatment-visit-configuration combinations to examine all parameter comparisons, but here we see that the model is sensitive to initialization for some combinations. This trend hold true across other fits. We can see from @tbl-initcompare that the grid-search initializaiton scheme also suffers form weaker predictive performance. For that reason, we recommend use of the forward-model MLEs as initializations for the backward model. 

```{r}
#| warning: false
#| label: tbl-initcompare
#| tbl-cap: "Comparison of predictive metrics for the MLE-initizlized (MLE) and grid-search initialized (Sensitivity) optimization techniques for replicates with a single true inoculation point (treatment level = 1) across visits and replicates."

library(gt)
# Summarise distance-weighted accuracy
dist_acc_sensitivity <- bind_rows(sensitivity_results$singlesource_acc) %>% 
  group_by(configuration) %>% 
  summarise(dist_acc = mean(dist_acc), .groups = "drop")

#Compute other prediction metrics
sensitivity_metrics <- sensitivity_results$singlesource_acc %>% 
  imap_dfr(~ multi_metrics(.x, truth = true_group, estimate = pred_group, ) %>% 
                    mutate(configuration = .y), .id = NULL) 

# Join with distance-weighted acc
sensitivity_metrics <- left_join(sensitivity_metrics, dist_acc_sensitivity, by = "configuration")%>% 
  select(-.estimator) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate)

left_join(mle_metrics, sensitivity_metrics, by = "configuration", suffix = c(".mle", ".sensitivity")) %>%
  mutate(configuration = c("2 x 2", "2 x 4", "4 x 2", "4 x 4")) %>% 
  select(configuration, contains("dist_acc"), contains("accuracy"), contains("kap"), contains("f_meas")) %>%
  gt(rowname_col = "configuration") %>%
  tab_spanner(
    label = "Distance-weighted Accuracy",
    columns = c(dist_acc.mle, dist_acc.sensitivity)
  ) %>%
  tab_spanner(
    label = "Accuracy",
    columns = c(accuracy.mle, accuracy.sensitivity)
  ) %>%
  tab_spanner(
    label = "Cohen's Kappa",
    columns = c(kap.mle, kap.sensitivity)
  ) %>%
  tab_spanner(
    label = md("$$F_1$$"),
    columns = c(f_meas.mle, f_meas.sensitivity)
  ) %>%
  cols_label(
    dist_acc.mle = "Informed",
    dist_acc.sensitivity = "Naive",
    accuracy.mle = "Informed",
    accuracy.sensitivity = "Naive",
    kap.mle = "Informed",
    kap.sensitivity = "Naive", 
    f_meas.mle = "Informed",
    f_meas.sensitivity = "Naive"
  ) %>% 
  fmt_number(decimals = 3)
  
```

# Conclusion

In this report, we introduced a source-detection model for the origin of Wheat Stripe Rust in an experimental setting. We assumed a single epidemic origin and an existing forward model that can accurately capture spread of the disease as determined by auto-infection and wind-based disperal vectors. Overall, we conclude that our model shows meaningful success in its ability to detect a single epidemic origin of Wheat Stripe Rust in this experimental setting. It is also possible that some latent disease dynamics are captured by cross-infection probabilities, but more work is needed to validate this claim. It is clear that the model fails when the single-source assumption is violated; however, there may be some distinguishing power between multiple sources which are placed closely together to form a single epidemic wavefront. Finally, we saw that our model is sensitive to initialization strategies. This is a common trait of EM-style algorithms. For that reason, we recommend fitting the forward and backward models separately to achieve the best predictive performance. 

Moving forward, we will assess our models ability to recover the true parameters in a simulation. 

# References

::: {#refs}
:::

# Figure Appendix

## 2 x 2

```{r}
possible_2by2 <- names(spat_plots)[str_detect(names(spat_plots), "stripe_4")]
for (i in possible_2by2) {
  print(spat_plots[[i]])
}
```
## 2 x 4

```{r}
possible_2by4 <- names(spat_plots)[str_detect(names(spat_plots), "stripe_8h")]
for (i in possible_2by4) {
  print(spat_plots[[i]])
}
```

## 4 x 2

```{r}
possible_4by2 <- names(spat_plots)[str_detect(names(spat_plots), "stripe_8v")]
for (i in possible_4by2) {
  print(spat_plots[[i]])
}
```

## 4 x 4

```{r}
possible_4by4 <- names(spat_plots)[str_detect(names(spat_plots), "stripe_16")]
for (i in possible_4by4) {
  print(spat_plots[[i]])
}
```